{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36325de5",
   "metadata": {},
   "source": [
    "# CCE2502 Assignment II - Logistic Regression\n",
    "### Year 2024-2025- Semester II\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89fd64",
   "metadata": {},
   "source": [
    "\n",
    "A number of functions are given, including the partially completed gradient descent optimizer.\n",
    "\n",
    "Please follow instructions in the assignment specifications document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1abc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not carry out any changes to the code in this cell\n",
    "\n",
    "def lr_loss_function(y_target,y_predicted):\n",
    "    \"\"\"\n",
    "    y_target: actual gold labels (vector of size(m,))\n",
    "    y_predicted : predicted probabilities for positive class (vector of size (m,))\n",
    "    returns categorical cross entropy loss\n",
    "    \"\"\"\n",
    "    m=y_target.shape[0]\n",
    "    L = -(y_target*np.log(y_predicted) + (1-y_target)*np.log(1-y_predicted)).sum()/m\n",
    "    return L\n",
    "\n",
    "\n",
    "def predict_y_prob(W,D):\n",
    "    \"\"\"\n",
    "    W is the weight vector [[w_0],[w_1],[w_2],...] of size [N+1,1]\n",
    "    where N is number of features, excluding bias\n",
    "    D is the feature matrix [m,N] where m is the number of examples\n",
    "    return a vector of size (m,1)\n",
    "    \"\"\"\n",
    "    m = D.shape[0]\n",
    "    bias = np.ones([m,1], dtype=float)\n",
    "    X = np.concatenate((bias,D),axis=1)\n",
    "    z = np.matmul(W.T,X.T)\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    #\n",
    "    return y.T\n",
    "\n",
    "def get_accuracy(W,X,y):\n",
    "    \"\"\"\n",
    "    W is the weight vector [[w_0],[w_1],[w_2],...] of size [N+1,1]\n",
    "    where N is number of features, excluding bias\n",
    "    X is the feature matrix [m,N] where m is the number of examples\n",
    "    y is the gold label vector of size (m,)\n",
    "    Computes and returns the accuracy in predicting the positive class and returns a scalar\n",
    "    \"\"\"\n",
    "    Y_pred = (predict_y_prob(W,X)>=0.5).astype(int)\n",
    "    #\n",
    "    return (Y_pred.ravel()==y).astype(int).sum()/y.shape[0]\n",
    "\n",
    "def display_results(model):\n",
    "    #\n",
    "    history_train_loss = model['history']['train_loss']\n",
    "    history_val_loss = model['history']['val_loss']\n",
    "    t=range(len(history_train_loss))\n",
    "    p1, p2 = plt.plot(t,history_train_loss,'b',t,history_val_loss,'r')\n",
    "    plt.legend([p1,p2], ['Train loss','Val loss'])\n",
    "    #\n",
    "    print('train acc : ', model['train_acc'])\n",
    "    print('val acc   : ', model['val_acc'])\n",
    "    #\n",
    "    print('Weights : ', model['weights'].ravel())\n",
    "    return 0\n",
    "\n",
    "# Use this function for plotting datasets as scatter graphs\n",
    "def data_scatter_plot(X,y):\n",
    "    \"\"\"\n",
    "    Plots a scatter graph for a 2-D dataset\n",
    "    X : Feature numpy array of size (m,N), \n",
    "            m=number of examples (rows)\n",
    "            N = number of features (columns)\n",
    "            If N>2, first two features taken in consideration\n",
    "    y : Output label numpy array of size (m,1) or (m,)\n",
    "    \"\"\"\n",
    "    mk ={0:'or', 1:'ob'}\n",
    "    plt.plot(X[:,0][y[:]==0] , X[:,1][y[:]==0], mk[0])\n",
    "    plt.plot(X[:,0][y[:]==1] , X[:,1][y[:]==1], mk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10343e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent algorithm that learns the model parameters\n",
    "# The update equations require the partial derivatives of the loss function\n",
    "# with respect to the weights or parameter of the model\n",
    "#\n",
    "def lr_train_model(X_data, \n",
    "                   y_data, \n",
    "                   reshuffle = 'yes',\n",
    "                   split_frac=0.8, \n",
    "                   init_weights=0.00001, \n",
    "                   no_iterations=2000,\n",
    "                   alpha = 0.001,\n",
    "                   phi = 0.001):\n",
    "    \"\"\"\n",
    "    X_data : data feature matrix (including bias x_0 = 1.0)\n",
    "    y_data : data output values\n",
    "    split_frac : data split fraction, we require the split to compute validation mse\n",
    "        if split_frac = -1 (or negative), then dataset is not split and train and val portions are the same\n",
    "    init_weights : initial weights (all same value)\n",
    "        if init_weights = -1 (or negative), then random weights are chosen\n",
    "    alpha : learning rate\n",
    "    phi : regularisation constant\n",
    "    W : weights vector\n",
    "    \"\"\"\n",
    "    #\n",
    "    idx = np.arange(0, y_data.shape[0])\n",
    "    if reshuffle == 'yes':\n",
    "        np.random.shuffle(idx)\n",
    "    X_data = np.copy(X_data[idx])\n",
    "    y_data = np.copy(y_data[idx])\n",
    "    #\n",
    "    if split_frac < 0.0:\n",
    "        y_train, y_val, X_train, X_val = y_data, y_data, X_data, X_data\n",
    "    else:\n",
    "        split = int(y_data.shape[0]*split_frac)\n",
    "        y_train, y_val, X_train, X_val  = y_data[0:split], y_data[split:], X_data[0:split], X_data[split:]\n",
    "    #\n",
    "    N = X_data.shape[1] # number of features\n",
    "    print(\"Number of input features :\",N)\n",
    "    if init_weights < 0.0:\n",
    "        W = 0.5*(np.random.rand((N+1))-0.5).reshape(N+1,1)\n",
    "    else:\n",
    "        W = np.full((N+1,1), init_weights) # initialise weight vectors to the same value\n",
    "    print(\"Initial Weights:\",W)\n",
    "    m=y_train.shape[0]\n",
    "    history_loss=[]\n",
    "    history_val_loss=[]\n",
    "    #\n",
    "    for itr in range(no_iterations):\n",
    "        y_pred_pr = predict_y_prob(W,X_train).ravel()\n",
    "        loss = lr_loss_function(y_train,y_pred_pr)\n",
    "        history_loss.append(loss)\n",
    "        history_val_loss.append(lr_loss_function(y_val,predict_y_prob(W,X_val).ravel()))\n",
    "        #\n",
    "        #\n",
    "        #######################################################################\n",
    "        #\n",
    "        # Add update equations in this section \n",
    "        # W[0] is the bias term (no regularisation)\n",
    "        #\n",
    "        #######################################################################\n",
    "    #\n",
    "    #\n",
    "    return {'weights':W, \n",
    "            'history':{'train_loss':history_loss,\n",
    "                                    'val_loss':history_val_loss},\n",
    "            'train_acc':get_accuracy(W,X_train,y_train),\n",
    "            'val_acc':get_accuracy(W,X_val,y_val)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3364f6",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbaf0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d104060",
   "metadata": {},
   "source": [
    "## TASK 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a2f3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7684e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "579c20e0",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36e6e0",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fed1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e8190f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9010d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
